> Status: Conclu√≠do üöÄ

# Objetivo
  
  O objetivo deste projeto √© utilizar uma base de dados do Kaggle para estudar a aplica√ß√£o e compara√ß√£o de modelos de Machine Learning. 
  
# Introdu√ß√£o

  Neste projeto temos uma base de dados de pacientes com algumas caracter√≠sticas coletadas e a ocorr√™ncia ou n√£o de diabetes. Os dados foram retirados do site: https://www.kaggle.com/uciml/pima-indians-diabetes-database?select=diabetes.csv

## Desenvolvimento

  A base de dados utilizados possui 768 registros com 9 atributos. N√£o foram encontrados valores NaN, entretanto, foi observada a exist√™ncia de atributos que estavam preenchidos com 0, o que n√£o fazia sentido. Insulina, Press√£o arterial e outros atributos zerados n√£o condizem com a realidade, provavelmente por algum motivo esses valores n√£o foram coletados e/ou registrados e foram preenchidos com zero ("hidden NaN values"). Ap√≥s uma verifica√ß√£o das distribui√ß√µes destes atributos, decidiu-se por preencher esses valores com o valor m√©dio do atributo utilizando a classe **SimpleImputer** do Scikit Learn antes do treinamento do modelo. Por ser um dataset muito pequeno, decidiu-se por n√£o excluir esses registros.   Para a separa√ß√£o dos dados foi utilizado o pacote **train_test_split**, utilizando a propor√ß√£o de 70/30.
  
  O primeiro algoritmo utilizado foi o GNB, *Gaussian Naive Bayes*, que gerou uma acur√°cia de 0,736, em outras palavras, o modelo conseguiu prever a ocorr√™ncia ou n√£o de diabetes em 73,6% dos casos. Em seguida foi utilizado o algoritmo de Random Forest, que gerou uma acur√°cia de *0,766*. O √∫ltimo algoritmo utilizado foi a regress√£o log√≠stica, que por sua vez conseguiu uma acur√°cia de *0,736*, convergindo apenas para valores acima de 200 itera√ß√µes.
  
  Abaixo segue um resumo dos resultados:
  
  |Algoritmo | Acur√°cia|
  |--- |---|
  |Gaussian Naive Bayes | 0,736|
  |Random Forest | 0,766
  |Logistic Regression | 0,736|
  
 # Conclus√µes
 Foram comparados alguns algoritmos de Machine Learning e escolhido um para fazer a previs√£o. Foi observada uma varia√ß√£o importante com a mudan√ßa dos par√¢metros dos algoritmos, em especial da Random Forest, que melhorou a acur√°cia de 0,73 para a,76 com o aumento do n√∫mero de estimadores. 
 
 # Melhorias futuras
 
 Normalizar as features antes de treinar o modelo e comparar os resultados;
 
 Criar uma fun√ß√£o que percorra automaticamente um range de estimadores para a Random Forest;
 
 Criar uma fun√ß√£o de itera√ß√µes para a regress√£o log√≠stica.

